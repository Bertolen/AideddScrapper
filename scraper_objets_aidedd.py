#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pour scraper les donnÃ©es d'objet magique D&D depuis aidedd.org
Extrait les donnÃ©es de la table avec l'id "liste", puis sauvegarde le tout en CSV
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
import sys
from urllib.parse import urljoin

def filter_empty_columns(items_data):
    """
    Filtre les colonnes sans en-tÃªte (vides) du dataset
    
    Args:
        items_data (list): DonnÃ©es des objets avec en-tÃªte
        
    Returns:
        list: DonnÃ©es filtrÃ©es sans colonnes vides
    """
    if not items_data or len(items_data) < 1:
        return items_data

    header_row = items_data[0]

    # Identifier les indices des colonnes avec un en-tÃªte non vide
    valid_columns = []
    for i, header in enumerate(header_row):
        if header and header.strip():  # En-tÃªte non vide
            valid_columns.append(i)
    
    # Filtrer toutes les lignes pour ne garder que les colonnes valides
    filtered_data = []
    for row in items_data:
        filtered_row = [row[i] if i < len(row) else '' for i in valid_columns]
        filtered_data.append(filtered_row)
    
    print(f"ğŸ”§ Filtrage des colonnes: {len(header_row)} â†’ {len(valid_columns)} colonnes conservÃ©es")
    
    return filtered_data

def extract_item_link(cell):
    """
    Extrait le lien d'un objet depuis une cellule de la table

    Args:
        cell: Cellule BeautifulSoup contenant potentiellement un lien
        
    Returns:
        str or None: URL relative de l'objet ou None si pas de lien
    """
    # Recherche d'un lien dans la cellule avec la classe "item"
    if cell.get('class') and 'item' in cell.get('class'):
        link = cell.find('a')
        if link and link.get('href'):
            return link.get('href')
    
    return None

def get_items_data(url):
    """
    RÃ©cupÃ¨re les donnÃ©es des objets

    Args:
        url (str): URL de la page Ã  scraper
        
    Returns:
        list: Liste des donnÃ©es des objets
    """
    print(f"Connexion Ã  {url}...")
    
    # Headers pour Ã©viter d'Ãªtre bloquÃ©
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # Envoi de la requÃªte
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print("Page rÃ©cupÃ©rÃ©e avec succÃ¨s!")
        print(f"Taille de la rÃ©ponse: {len(response.content)} bytes")
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Recherche de la table avec l'id "liste"
        table = soup.find('table', {'id': 'liste'})
        
        if not table:
            print("âŒ Erreur: Table avec id 'liste' non trouvÃ©e!")
            return []
        
        print("âœ… Table 'liste' trouvÃ©e!")
        
        # Extraction des donnÃ©es
        items_data = []
        item_links = []

        # Recherche des lignes de donnÃ©es
        rows = table.find_all('tr')
        print(f"Nombre de lignes trouvÃ©es: {len(rows)}")
        
        # DÃ©terminer l'en-tÃªte
        header_row = None
        data_rows = []
        item_links_data = []  # Pour stocker les liens avec les donnÃ©es
        
        for i, row in enumerate(rows):
            cells = row.find_all(['th', 'td'])
            if not cells:
                continue
                
            # Si c'est la premiÃ¨re ligne avec des donnÃ©es ou contient des th, c'est probablement l'en-tÃªte
            if header_row is None and (cells[0].name == 'th' or i == 0):
                header_row = [cell.get_text(strip=True) for cell in cells]
                # Ajout de la colonne lien Ã  la fin
                header_row.append('Lien_Description')
                print(f"En-tÃªte dÃ©tectÃ©: {header_row}")
            else:
                # Extraction des donnÃ©es de la ligne
                row_data = []
                item_link = None
                
                for j, cell in enumerate(cells):
                    # RÃ©cupÃ©ration du texte
                    text = cell.get_text(strip=True)
                    row_data.append(text)

                    # VÃ©rification si cette cellule contient le lien de l'objet (cellule avec classe "item")
                    if not item_link:
                        link = extract_item_link(cell)
                        if link:
                            item_link = link

                if any(row_data):  # Si la ligne contient des donnÃ©es
                    data_rows.append(row_data)
                    item_links_data.append(item_link)  # Associer le lien aux donnÃ©es

        # Si pas d'en-tÃªte dÃ©tectÃ©, crÃ©er un en-tÃªte gÃ©nÃ©rique
        if header_row is None and data_rows:
            header_row = [f"Colonne_{i+1}" for i in range(len(data_rows[0]))]
            header_row.append('Lien_Description')
            print(f"En-tÃªte gÃ©nÃ©rique crÃ©Ã©: {header_row}")
        
        # Ajout de l'en-tÃªte aux donnÃ©es
        if header_row:
            items_data.append(header_row)
        
        print("âš ï¸  Cela peut prendre quelques minutes...")

        # Extraction du lien pour chaque objet
        base_url_for_items = "https://www.aidedd.org/magic-item/"

        for i, (row_data, item_link) in enumerate(zip(data_rows, item_links_data)):
            print(f"ğŸ“œ Traitement de l'objet {i+1}/{len(data_rows)}: {row_data[1] if len(row_data) > 1 else 'N/A'}")
            
            # Ajout du lien de description complet Ã  la fin
            if item_link:
                full_item_url = urljoin(base_url_for_items, item_link)
                row_data.append(full_item_url)
            else:
                row_data.append('')  # Lien vide si pas de lien trouvÃ©
            
            items_data.append(row_data)

        print(f"\nâœ… {len(data_rows)} objets extraits (+ en-tÃªte)")

        # Filtrer les colonnes vides avant de retourner les donnÃ©es
        items_data = filter_empty_columns(items_data)

        return items_data

    except requests.RequestException as e:
        print(f"âŒ Erreur lors de la requÃªte: {e}")
        return []
    except Exception as e:
        print(f"âŒ Erreur inattendue: {e}")
        return []

def save_to_csv(data, filename):
    """
    Sauvegarde les donnÃ©es dans un fichier CSV
    
    Args:
        data (list): DonnÃ©es Ã  sauvegarder
        filename (str): Nom du fichier CSV
    """
    if not data:
        print("âŒ Aucune donnÃ©e Ã  sauvegarder!")
        return False
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile, delimiter=',', quotechar='"', quoting=csv.QUOTE_ALL)
            
            for row in data:
                writer.writerow(row)
        
        print(f"âœ… DonnÃ©es sauvegardÃ©es dans '{filename}'")
        print(f"   {len(data)} lignes Ã©crites")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde: {e}")
        return False

def main():
    """Fonction principale"""
    print("=== SCRAPER AIDEDD.ORG - OBJETS D&D ===\n")
    
    # URL de base
    url = "https://www.aidedd.org/magic-item/fr/"

    # Nom du fichier de sortie
    output_file = "objets_dnd_aidedd.csv"

    # Extraction des donnÃ©es
    print("ğŸ” Extraction des donnÃ©es...")
    items_data = get_items_data(url)

    if not items_data:
        print("âŒ Ã‰chec de l'extraction des donnÃ©es")
        return False
    
    # Sauvegarde
    print("\nğŸ’¾ Sauvegarde des donnÃ©es...")
    success = save_to_csv(items_data, output_file)
    
    if success:
        print(f"\nğŸ‰ Scraping terminÃ© avec succÃ¨s!")
        print(f"ğŸ“ Fichier crÃ©Ã©: {output_file}")
        
        # Affichage d'un aperÃ§u
        if items_data and len(items_data) > 1:
            print(f"\nğŸ“‹ AperÃ§u des donnÃ©es (3 premiÃ¨res lignes):")
            for i, row in enumerate(items_data[:3]):
                if i == 0:
                    print(f"  En-tÃªte: {row[:5]}... [+{len(row)-5} colonnes]")
                else:
                    print(f"  {i}. {row[1] if len(row) > 1 else 'N/A'}")
    else:
        print("\nâŒ Ã‰chec du scraping")
        return False
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ArrÃªt du script par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Erreur fatale: {e}")
        sys.exit(1)